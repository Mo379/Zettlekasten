# Daily Summary
----
Use this note to save anything you want for the day


# To do 
---
- [x] Open Daily Note
- [ ] Sort out the speed benchmarking
	- [ ] do the bench marking
	- [ ] visualise the scaling law | plot an extrapolation
	- [ ] possible expansion of the benchmark to use different cutoffs
- [ ] exploring the mace optimiser | show parameter evolution in time
- [ ] debugging nans
- [ ] Do some more note reading


# Jot Things down here
---
#### Mace Issues

mace is a very nice network but last I checked it does not work properly in our implementation, we  try to simulate larger systems but we only seem to obtain qualitative accuracy and nothing else, we need to be able to obtain quantitative accuracy else we will not have anything to show for it.

The network has been show to be very much more accurate than other networks but still remains to be so for our experiments, the question remains is this because of our
own ignorance about the problem of application we're attempting ? 

Out goal is to train mace on the fragmented data (fragmented proteins) and hope that the system will be able to perform the same when we inference on larger systems outside of the distribution of our dataset in terms of system size. This does not occur at the moment


#### Observations

The main issue is that as soon as the input size system/protein is greater than the training dataset's average we get bad network outputs.
- Meaning that we do not get a generalisation on the fundamental physics but instead we learn how individual fragments behave.

We also observe that as the protein folds we get a different local neighbourhood that is within the same size of the distribution but the network still outputs bad results.
- This can still be attributed to not learning fundamental physics but instead learning 'fragment physics', thus because this new 'folded protein fragment' is not in the dataset, we end up getting weird results again.

So the question is how can we get the current network to work properly ?
1) For a new protein, break it down into fragments using the same strategy of dataset generation
2) The network gets each of those fragments and predicts on them
3) We assign those values to the network

#### How to get a generalised dataset

1) Figure out a new way of training such that the network learns the fundamental physics, interactions between arbitrary atoms in the fragment
	1) The dataset generation strategy will have to produce N-order body interaction forces using DFT
	2) The network learns not about a single fragments but about the whole idea of molecular interactions between individual atoms.
2) Figure out a pipeline to feed the network new DFT interactions
	1) The network outputs a confidence about its own predictions for a specific N-order body interaction
	2) The network can then prompt a DTF API with the atom types and positions to get a short simulation, that simulation can be added to the dataset
		1) potential issue with replicate datapoints (atoms and distances), maybe include a filtration mechanism of allowed ranges (distances) and 
	3) That is added to the dataset and the network proceeds to learn from the pile.