#### Meta information
23-02-02, 11:32
Status: #idea
Tags: [[TheValue]] [[MathematicalFormalismOfTheBellmanEquations]] [[MathematicalFormalismOfOptimalityInRL]] [[InterpretingTheStateAactionValueFunction]]





# OptimisingRLPolicies

- The Value function can be used to optice the policy.
- We have to concider the value functions for optimal and non optimal policies,
- This optimal policy maximises the values of all initial states,
- The non optimal policy indicates that there is a state for which the value is not the best, and so for that state there is a different optimal policy that give a higher value,
- Through the chaining nature of the bellman equations, the new optimal policy for that state, improve the values of all previous states
- So the new optimal policy improves the value of at least one initialising state, so it increases the return obtained on an initial state.





# References
